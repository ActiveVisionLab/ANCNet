<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
}	
h1 {
    font-weight:300;
}

.disclaimerbox {
    background-color: #eee;		
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

a:link,a:visited
{
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
  }

  td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
  }

  .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<html>
<head>
<title>Correspondence Networks with Adaptive Neighbourhood Consensus</title>
<meta property="og:image" content="./asset/splash.png"/>
<meta property="og:title" content="Correspondence Networks with Adaptive Neighbourhood Consensus. In CVPR, 2020." />
</head>

<body>
<br>
<center>
    <span style="font-size:42px">Correspondence Networks with Adaptive Neighbourhood Consensus</span><br>

    <table align=center width=1000px>
        <tr>
            <td align=center width=1000px>
                <span style="font-size:22px"><a href="https://lishuda.wordpress.com/">Shuda Li<sup>1</sup>*</a></span> &emsp;
                <span style="font-size:22px"><a href="http://www.hankai.org/">Kai Han<sup>2</sup>*</a></span> &emsp;
                <span style="font-size:22px"><a href="https://www.robots.ox.ac.uk/~costain/">Theo W. Costain<sup>1</sup></a></span> &emsp;
                <span style="font-size:22px"><a href="">Henry Howard-Jenkins<sup>1</sup></a></span> &emsp;
                <span style="font-size:22px"><a href="http://www.robots.ox.ac.uk/~victor/">Victor Prisacariu<sup>1</sup></a></span>
            </td>
        </tr>
    </table>


    <span style="font-size:21px">Active Vision Lab<sup>1</sup> & Visual Geometry Group<sup>2</sup></span> <br>
    <span style="font-size:22px">Department of Engineering Science, University of Oxford</span>
    <br>
    <span style="font-size:20px">* indicates equal contribution</span>
    <br>
    <br>

    <table align=center width=900px>
        <tr>
            <td align=center width=300px>
                <span style="font-size:22px"><a href='./asset/cvpr2020_anc_net.pdf'> Paper [CVPR 2020]</a></span> &emsp;&emsp;
                <span style="font-size:22px"><a href='https://github.com/ActiveVisionLab/ANCNet'> Code [PyTorch]</a></span>

            </td>
        </tr>
    </table>
</center>
<br>

<table align=center width=800px>
    <tr>
        <td width=400px>
            <center>
                <img class="rounded" src = "./asset/splash.png" height="240px"></img>
                <br>
            </center>
        </td>
    </tr>
</table>

<br>
<hr>

<table align=center width=900px>
    <center><h1>Abstract</h1></center>
    <p>
        In this paper, we tackle the task of establishing dense visual correspondences between images containing objects of the same category.
        This is a challenging task due to large intra-class variations and a lack of dense pixel level annotations.
        We propose a convolutional neural network architecture, called adaptive neighbourhood consensus network (ANC-Net), that can be trained end-to-end with sparse key-point annotations, to handle this challenge.
        At the core of ANC-Net is our proposed non-isotropic 4D convolution kernel, which forms the building block for the adaptive neighbourhood consensus module for robust matching.
        We also introduce a simple and efficient multi-scale self-similarity module in ANC-Net to make the learned feature robust to intra-class variations.
        Furthermore, we propose a novel orthogonal loss that can enforce the one-to-one matching constraint.
        We  thoroughly evaluate the effectiveness of our method on various benchmarks, where it substantially outperforms state-of-the-art methods. 
    </p>
</table>
<hr>

<table align=center width=900px>
    <center><h1>BibTex</h1></center>
        <pre>
            <span class="inner-pre" style="font-size: 14px">
  @inproceedings{Li2020Correspondence,
    author    = {Shuda Li and Kai Han and Theo W. Costain and Henry Howard-Jenkins and Victor Prisacariu},
    title     = {Correspondence Networks with Adaptive Neighbourhood Consensus},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2020},
  }
</span>
        </pre>
</table>
<hr>

<table align=center width=1000px>
    <tr>
        <td width=400px>
            <left>
            <center><h1>Acknowledgments</h1></center>
            We gratefully acknowledge the support of the European Commission Project Multiple-actOrs Virtual EmpathicCARegiver for the Elder (MoveCare) and the EPSRC Programme Grant Seebibyte  EP/M013774/1.
            </left>
        </td>
    </tr>
</table>

<br><br>
<p style="text-align:center;font-size:16px;">
    Webpage template borrowed from <a href="https://richzhang.github.io/splitbrainauto/">Split-Brain Autoencoders, CVPR 2017</a>.
</p>
</body>
</html>

